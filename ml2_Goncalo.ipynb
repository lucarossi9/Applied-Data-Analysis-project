{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2\n",
    "\n",
    "---\n",
    "By the end of this milestone we expect to be able to:\n",
    "1. Load data from json file\n",
    "2. Filter the dataset\n",
    "3. Do a sentimental analisys of the quotes\n",
    "4. Do a sensitivity analisys; \n",
    "5. Create meaningful visualizations to analyze the data;\n",
    "---\n",
    "\n",
    "\n",
    "## Context\n",
    "\n",
    "In this project, we are going to analyze a data set of quotes from New York Times in order to cluster the authors of the quotes by common opinions. \n",
    "\n",
    "Oftentimes, we would like to know someone's opinion about a certain missing topic. In that case, if we have a cluster  made  of people with similar opinions in a variety of topics, we can try to infer their opinion on the missing topic based on the opinion of the majority of his/her cluster.THis is a process similar to the KKN algorithm\n",
    "\n",
    "The data set is presented as an aggregated set of unique quotations with the most likely speaker. Each unique quotation occurs only once in this version of the data and the probabilities of the candidate speakers to which the quotation can be attributed are aggregated over all occurrences of the quotation. This version of the data is a minimal - but complete - list of attributed quotations that is aimed at users who only require quotation-speaker attributions, but no individual contexts for these quotations from the original articles.\n",
    "\n",
    "\n",
    "\n",
    "## The data\n",
    "\n",
    "We are provided with a compressed `.bz2` json file containing one row per quote. \n",
    "The `.json` has the following fields:\n",
    "\n",
    " - `quoteID`: Primary key of the quotation (format: \"YYYY-MM-DD-{increasing int:06d}\")\n",
    " - `quotation`: Text of the longest encountered original form of the quotation\n",
    " - `date`: Earliest occurrence date of any version of the quotation\n",
    " - `phase`: Corresponding phase of the data in which the quotation first occurred (A-E)\n",
    " - `probas`: Array representing the probabilities of each speaker having uttered the quotation.\n",
    "      The probabilities across different occurrences of the same quotation are summed for\n",
    "      each distinct candidate speaker and then normalized\n",
    "      - `proba`: Probability for a given speaker\n",
    "      - `speaker`: Most frequent surface form for a given speaker in the articles where the quotation occurred\n",
    " - `speaker`: Selected most likely speaker. This matches the the first speaker entry in `probas`\n",
    " - `qids`: Wikidata IDs of all aliases that match the selected speaker\n",
    " - `numOccurrences`: Number of time this quotation occurs in the articles\n",
    " - `urls`: List of links to the original articles containing the quotation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports we may need\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ujson as json\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the quotes files\n",
    "\n",
    "Here I individualy loaded the data of each year by chunks because we can't fully load the entire set, is just to much memory allocation, so my plan is a \"Divide and Conquer\" method, where we read the first 10000 quotes, work on them, save the results and then we go get 10000 more quotes. Onether thing, I don't know if we are suposed to download the files because they are a bit large, but I dicided to download them just to give them a try\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read YYYY quotes file\n",
    "\n",
    "chunks_2008 = pd.read_json(\"data/quotes-2008.json.bz2\",lines=True, chunksize = 10000, compression = \"bz2\")\n",
    "chunks_2009 = pd.read_json(\"data/quotes-2009.json.bz2\",lines=True, chunksize = 10000, compression = \"bz2\")\n",
    "chunks_2010 = pd.read_json(\"data/quotes-2010.json.bz2\",lines=True, chunksize = 10000, compression = \"bz2\")\n",
    "chunks_2011 = pd.read_json(\"data/quotes-2011.json.bz2\",lines=True, chunksize = 10000, compression = \"bz2\")\n",
    "chunks_2012 = pd.read_json(\"data/quotes-2012.json.bz2\",lines=True, chunksize = 10000, compression = \"bz2\")\n",
    "chunks_2013 = pd.read_json(\"data/quotes-2013.json.bz2\",lines=True, chunksize = 10000, compression = \"bz2\")\n",
    "chunks_2014 = pd.read_json(\"data/quotes-2014.json.bz2\",lines=True, chunksize = 10000, compression = \"bz2\")\n",
    "chunks_2015 = pd.read_json(\"data/quotes-2015.json.bz2\",lines=True, chunksize = 10000, compression = \"bz2\")\n",
    "chunks_2016 = pd.read_json(\"data/quotes-2016.json.bz2\",lines=True, chunksize = 10000, compression = \"bz2\")\n",
    "chunks_2017 = pd.read_json(\"data/quotes-2017.json.bz2\",lines=True, chunksize = 10000, compression = \"bz2\")\n",
    "chunks_2018 = pd.read_json(\"data/quotes-2018.json.bz2\",lines=True, chunksize = 10000, compression = \"bz2\")\n",
    "chunks_2019 = pd.read_json(\"data/quotes-2019.json.bz2\",lines=True, chunksize = 10000, compression = \"bz2\")\n",
    "chunks_2020 = pd.read_json(\"data/quotes-2020.json.bz2\",lines=True, chunksize = 10000, compression = \"bz2\")\n",
    "\n",
    "chunks_YYYY = [chunks_2008, chunks_2009, chunks_2010, chunks_2011, chunks_2012, chunks_2013, chunks_2014, chunks_2015, chunks_2016, chunks_2017, chunks_2018, chunks_2019, chunks_2020]\n",
    "\n",
    "years_list= [\"2008\",\"2009\",\"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\",\"2019\", \"2020\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to do some tests on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I was curious to find how many quotes each file had, so I implemented a function to count the number of chunks with 10000 quotes each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36653/3923733802.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myears_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/quotes-\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0myear\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\".json.bz2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bz2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Year: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"with\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"partitions of 10,000 quotes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "#counts the number of partitions with 10000 quotes of each year \n",
    "count = 0\n",
    "i = 0\n",
    "for year in years_list:\n",
    "    for chunks in pd.read_json(\"data/quotes-\"+ year +\".json.bz2\",lines=True, chunksize = 10000, compression = \"bz2\"):\n",
    "        count[i] = count + 1\n",
    "    print(\"Year: \", year, \"with\", count[i], \"partitions of 10,000 quotes\")\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I was curious to know, how many quotes that month had in a certain year, and at first I thought it was a good way to divide and analyse the quotes by month, but after running the function and reading again what the \"date\" was, I realized that propably wasn't very useful ideia because the date that is shown in the \"date\" column is the last date that the quote was mention (i.e if the same quote was mentioned in April and December, the date will be the December one), this makes that we only have quotes in the last months, so the data is not evenly distributed by months as I first expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Quotes_Year_month(year, month):\n",
    "    chunk_month = []\n",
    "    chunks_Year = pd.read_json(\"data/quotes-\"+ year +\".json.bz2\",lines=True, chunksize = 10000, compression = \"bz2\")\n",
    "    for chunk in chunks_Year:\n",
    "        chunk_month.append(chunk[pd.DatetimeIndex(chunk['date']).month == month])\n",
    "    return chunk_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_month = Quotes_Year_month(\"2008\", 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-09-15-000037</td>\n",
       "      <td>13th watered heavily up to the ridge, then roc...</td>\n",
       "      <td>gerry byrne</td>\n",
       "      <td>[Q3104325, Q959780]</td>\n",
       "      <td>2008-09-15 16:29:27</td>\n",
       "      <td>1</td>\n",
       "      <td>[[gerry byrne, 0.4065], [howard clark, 0.3118]...</td>\n",
       "      <td>[http://belfasttelegraph.co.uk/sport/golf/amer...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-09-16-000159</td>\n",
       "      <td>80 per cent towards the basis of an agreement.</td>\n",
       "      <td>mario cossio</td>\n",
       "      <td>[Q9029123]</td>\n",
       "      <td>2008-09-16 01:12:16</td>\n",
       "      <td>2</td>\n",
       "      <td>[[mario cossio, 0.4073], [evo morales, 0.247],...</td>\n",
       "      <td>[http://channelnewsasia.com/stories/afp_world/...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-09-03-000269</td>\n",
       "      <td>a challenge for the u.s. and the world,</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>2008-09-03 19:10:55</td>\n",
       "      <td>2</td>\n",
       "      <td>[[None, 0.5239], [mikheil saakashvili, 0.4761]]</td>\n",
       "      <td>[http://warisboring.com/?p=1339]</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2008-09-22-000262</td>\n",
       "      <td>a child's head is the heaviest part of his bod...</td>\n",
       "      <td>michael turner</td>\n",
       "      <td>[Q1372347, Q1372443, Q15969753, Q1929622, Q208...</td>\n",
       "      <td>2008-09-22 14:01:20</td>\n",
       "      <td>1</td>\n",
       "      <td>[[michael turner, 0.5583], [None, 0.4417]]</td>\n",
       "      <td>[http://cnn.com/2008/HEALTH/family/09/22/accid...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008-09-25-000330</td>\n",
       "      <td>a core taking over our education system?</td>\n",
       "      <td>thomas muthee</td>\n",
       "      <td>[Q7792644]</td>\n",
       "      <td>2008-09-25 20:39:19</td>\n",
       "      <td>1</td>\n",
       "      <td>[[thomas muthee, 0.5289], [None, 0.4711]]</td>\n",
       "      <td>[http://woai.com/content/blogs/wiccan/story.as...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4641312</th>\n",
       "      <td>2008-09-16-071984</td>\n",
       "      <td>you never want to go to somebody else's place ...</td>\n",
       "      <td>jim scherr</td>\n",
       "      <td>[Q15820748]</td>\n",
       "      <td>2008-09-16 05:17:56</td>\n",
       "      <td>1</td>\n",
       "      <td>[[jim scherr, 0.431], [peter ueberroth, 0.2946...</td>\n",
       "      <td>[http://gazette.com/sports/usoc_40594___articl...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4641317</th>\n",
       "      <td>2008-09-26-069663</td>\n",
       "      <td>you shouldn't look at [ your balance ] every d...</td>\n",
       "      <td>carmen wong ulrich</td>\n",
       "      <td>[Q5043616]</td>\n",
       "      <td>2008-09-26 22:04:25</td>\n",
       "      <td>1</td>\n",
       "      <td>[[carmen wong ulrich, 0.5583], [None, 0.4417]]</td>\n",
       "      <td>[http://usnews.com/blogs/new-money/2008/9/26/o...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4641320</th>\n",
       "      <td>2008-09-30-070962</td>\n",
       "      <td>you will be an accountant for the rest of your...</td>\n",
       "      <td>you long</td>\n",
       "      <td>[Q45476726, Q45678948]</td>\n",
       "      <td>2008-09-30 00:15:04</td>\n",
       "      <td>1</td>\n",
       "      <td>[[you long, 0.8172], [None, 0.1828]]</td>\n",
       "      <td>[http://messagefromthemuse.typepad.com/message...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4641321</th>\n",
       "      <td>2008-09-16-072287</td>\n",
       "      <td>you'd like to think if you're going to make a ...</td>\n",
       "      <td>mike nolan</td>\n",
       "      <td>[Q3249544, Q6848203]</td>\n",
       "      <td>2008-09-16 14:44:20</td>\n",
       "      <td>1</td>\n",
       "      <td>[[mike nolan, 0.2222], [shawntae spencer, 0.20...</td>\n",
       "      <td>[http://pressdemocrat.com/article/20080915/spo...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4641325</th>\n",
       "      <td>2008-09-02-084246</td>\n",
       "      <td>you're damn right you are, now go ahead and kn...</td>\n",
       "      <td>vijay singh</td>\n",
       "      <td>[Q16122677, Q21170857, Q24005696, Q331942, Q35...</td>\n",
       "      <td>2008-09-02 04:08:12</td>\n",
       "      <td>1</td>\n",
       "      <td>[[vijay singh, 0.5488], [None, 0.4512]]</td>\n",
       "      <td>[http://pgatour.com/2008/tournaments/r505/09/0...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1257759 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   quoteID                                          quotation  \\\n",
       "1        2008-09-15-000037  13th watered heavily up to the ridge, then roc...   \n",
       "3        2008-09-16-000159     80 per cent towards the basis of an agreement.   \n",
       "4        2008-09-03-000269            a challenge for the u.s. and the world,   \n",
       "5        2008-09-22-000262  a child's head is the heaviest part of his bod...   \n",
       "8        2008-09-25-000330           a core taking over our education system?   \n",
       "...                    ...                                                ...   \n",
       "4641312  2008-09-16-071984  you never want to go to somebody else's place ...   \n",
       "4641317  2008-09-26-069663  you shouldn't look at [ your balance ] every d...   \n",
       "4641320  2008-09-30-070962  you will be an accountant for the rest of your...   \n",
       "4641321  2008-09-16-072287  you'd like to think if you're going to make a ...   \n",
       "4641325  2008-09-02-084246  you're damn right you are, now go ahead and kn...   \n",
       "\n",
       "                    speaker  \\\n",
       "1               gerry byrne   \n",
       "3              mario cossio   \n",
       "4                      None   \n",
       "5            michael turner   \n",
       "8             thomas muthee   \n",
       "...                     ...   \n",
       "4641312          jim scherr   \n",
       "4641317  carmen wong ulrich   \n",
       "4641320            you long   \n",
       "4641321          mike nolan   \n",
       "4641325         vijay singh   \n",
       "\n",
       "                                                      qids  \\\n",
       "1                                      [Q3104325, Q959780]   \n",
       "3                                               [Q9029123]   \n",
       "4                                                       []   \n",
       "5        [Q1372347, Q1372443, Q15969753, Q1929622, Q208...   \n",
       "8                                               [Q7792644]   \n",
       "...                                                    ...   \n",
       "4641312                                        [Q15820748]   \n",
       "4641317                                         [Q5043616]   \n",
       "4641320                             [Q45476726, Q45678948]   \n",
       "4641321                               [Q3249544, Q6848203]   \n",
       "4641325  [Q16122677, Q21170857, Q24005696, Q331942, Q35...   \n",
       "\n",
       "                       date  numOccurrences  \\\n",
       "1       2008-09-15 16:29:27               1   \n",
       "3       2008-09-16 01:12:16               2   \n",
       "4       2008-09-03 19:10:55               2   \n",
       "5       2008-09-22 14:01:20               1   \n",
       "8       2008-09-25 20:39:19               1   \n",
       "...                     ...             ...   \n",
       "4641312 2008-09-16 05:17:56               1   \n",
       "4641317 2008-09-26 22:04:25               1   \n",
       "4641320 2008-09-30 00:15:04               1   \n",
       "4641321 2008-09-16 14:44:20               1   \n",
       "4641325 2008-09-02 04:08:12               1   \n",
       "\n",
       "                                                    probas  \\\n",
       "1        [[gerry byrne, 0.4065], [howard clark, 0.3118]...   \n",
       "3        [[mario cossio, 0.4073], [evo morales, 0.247],...   \n",
       "4          [[None, 0.5239], [mikheil saakashvili, 0.4761]]   \n",
       "5               [[michael turner, 0.5583], [None, 0.4417]]   \n",
       "8                [[thomas muthee, 0.5289], [None, 0.4711]]   \n",
       "...                                                    ...   \n",
       "4641312  [[jim scherr, 0.431], [peter ueberroth, 0.2946...   \n",
       "4641317     [[carmen wong ulrich, 0.5583], [None, 0.4417]]   \n",
       "4641320               [[you long, 0.8172], [None, 0.1828]]   \n",
       "4641321  [[mike nolan, 0.2222], [shawntae spencer, 0.20...   \n",
       "4641325            [[vijay singh, 0.5488], [None, 0.4512]]   \n",
       "\n",
       "                                                      urls phase  \n",
       "1        [http://belfasttelegraph.co.uk/sport/golf/amer...     A  \n",
       "3        [http://channelnewsasia.com/stories/afp_world/...     A  \n",
       "4                         [http://warisboring.com/?p=1339]     A  \n",
       "5        [http://cnn.com/2008/HEALTH/family/09/22/accid...     A  \n",
       "8        [http://woai.com/content/blogs/wiccan/story.as...     A  \n",
       "...                                                    ...   ...  \n",
       "4641312  [http://gazette.com/sports/usoc_40594___articl...     A  \n",
       "4641317  [http://usnews.com/blogs/new-money/2008/9/26/o...     A  \n",
       "4641320  [http://messagefromthemuse.typepad.com/message...     A  \n",
       "4641321  [http://pressdemocrat.com/article/20080915/spo...     A  \n",
       "4641325  [http://pgatour.com/2008/tournaments/r505/09/0...     A  \n",
       "\n",
       "[1257759 rows x 9 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_month_DataFrame = pd.concat(chunk_month)\n",
    "chunk_month_DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to get chunks after a certain chunk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_and_transform(year, chunk_number, chunksize, f):\n",
    "    iterator = pd.read_json(\"data/quotes-\"+ year +\".json.bz2\",lines=True, chunksize = chunksize, compression = \"bz2\")\n",
    "    for j in range(chunk_number):\n",
    "        iterator.__next__()\n",
    "    chunk_partition = iterator.__next__()\n",
    "    specific_chunk = f(chunk_partition)\n",
    "    return specific_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.io.json._json.JsonReader at 0x7f798ca807f0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "Obama = get_chunk_and_transform(\"2008\", 0, 5, lambda x: x[\"speaker\"])\n",
    "\n",
    "Obama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I wanted to do a histogram with the occurencies of the main speakers of a certain year (i tried for the year of 2008), and since that is so many quotes I decided to do the histogram based on a sample of 10% of the data set, and in order to have a cleaner visualization of the main speakers, I only ploted the bars of speakers with more than 500 quotes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36653/3222022425.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mchunk_sample_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks_2008\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mchunk_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mchunk_sample_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"speaker\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda3/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0mlines_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda3/lib/python3.8/bz2.py\u001b[0m in \u001b[0;36mread1\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda3/lib/python3.8/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda3/lib/python3.8/_compression.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0mrawblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#count occurencies of speakers\n",
    "init = True\n",
    "i = 0\n",
    "chunk_sample_list = []\n",
    "for chunk in chunks_2008:\n",
    "    chunk_sample = chunk.sample(frac = 0.1)\n",
    "    chunk_sample_list.append(chunk_sample[\"speaker\"])\n",
    "    i = i+ 1\n",
    "    print(i)\n",
    "    \n",
    "speakers_counts = pd.concat(chunk_sample_list)\n",
    "speakers_counts = speakers_counts.value_counts()\n",
    "speakers = speakers_counts[speakers_counts > 500]\n",
    "speakers = pd.DataFrame(speakers)\n",
    "ax = sns.barplot(x=\"speaker\", y=speakers.index, data=speakers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
